<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[线性表]]></title>
      <url>%2F2017%2F05%2F03%2F%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;using namespace std;#define LIST_INIT_SIZE 100 // 线性储存空间的初始分配量#define LISTINCREMENT 10 //线性储存空间的分配鞥量（当线性表的空间不够用时，进行增加的单位）typedef int ElemType; // 数据元素的类型，//定义线性表元素typedef struct &#123; ElemType *elem; // 储存空间的基地址 int length; //当前线性表的长度 int liestsize; //当前分配的储存容量&#125;SqlList;//定义线性表的操作// createint initList(SqlList &amp;L) &#123; L.elem = (ElemType *)malloc(LIST_INIT_SIZE*sizeof(ElemType));// 开辟一个储存空间，把这个储存空间的基地址赋值给elem if (!L.elem) &#123; return -1;// 分配失败 &#125; L.length = 0;//当前长度 L.liestsize = LIST_INIT_SIZE;//当前分配量 return 0;&#125;//插入元素int listInsert(SqlList &amp;L, int i, ElemType e) &#123; //判断插入位置是否合法 if (i&lt;1 || i&gt;L.length) return -1; //判断内存是否够用 if (L.length &gt;= L.liestsize) &#123; ElemType *newBase = (ElemType *)realloc(L.elem, (L.liestsize + LISTINCREMENT)*sizeof(ElemType));//增加内存 if (!newBase) return -1;//储存空间分配失败 L.elem = newBase;//新基址，疑问：需要指明吗？ L.liestsize += LISTINCREMENT; &#125; ElemType *q, *p;// q = &amp;(L.elem[i - 1]);//取出要插入位置的地址，下标是0开始 for (p = &amp;(L.elem[L.length - 1]); p &gt;= q; --p) &#123; *(p + 1) = *p; &#125; *q = e; ++L.length; return 0;&#125;//删除元素int listDelete(SqlList &amp;L, int i, ElemType &amp;e) &#123; //判断删除位置是否合法 if (i&lt;1 || i&gt;L.length) return -1; //删除操作 ElemType *q, *p; p = &amp;(L.elem[i - 1]); e = *p; q = &amp;(L.elem[L.length - 1]); for (++p; p &lt; q; ++p) &#123; *(p - 1) = *p; &#125; --L.length; return 0;&#125;// 查找元素（按值查找）int LocateElem(SqlList L, ElemType x) &#123; int pos = -1;//用来记录查找到的位置 for (int i = 0; i &lt; L.length; i++) &#123; if (L.elem[i] == x) pos = i; &#125; return pos;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python学习第一章]]></title>
      <url>%2F2017%2F05%2F01%2Fpython-study-first-day%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# coding=utf-8import refrom urllib import urlopenfrom urllib2 import HTTPErrorfrom bs4 import BeautifulSoup# 周密的异常处理让代码复用# 获取网页titledef getTitle(url): try: # 检查URL是否正常工作 html = urlopen(url) except HTTPError as e: return None try: # 检查便签是否存在 bsObj = BeautifulSoup(html.read(), "html.parser") # 可有单个参数：content， 解析方式， 向网页提交内容：data title = bsObj.body.head # 了解HTML的标签格式 except AttributeError as e: return None return title# 训练使用 bs4 findAll 和 find# 通过属性查找标签# findAll()的参数设置# 当网站管理员对网页内容稍加更改，代码是否会失效# 把请求头设置成移动设备在线# 寻找隐藏在JavaScript里面的信息# findAll(tag, attributes, recursive, text, limit, keywords)# recursive 是 bool 值， 若为 true 则可以查找子标签的子标签， 否则只能查到一级标签， 默认true# text 是用内容去匹配 而不是标签属性# limit 范围限制参数 网页中前x项# keyword 选择具有指定属性的标签# find(tag, attributes, recursive, text, keywords)def useFind(url): try: html = urlopen(url) except HTTPError as e: return None try: bsObj = BeautifulSoup(html.read(), "html.parser") nameList = bsObj.findAll("span", &#123;"class": &#123;"green", "red"&#125;&#125;) # bsObj.findAll(text = "jerry") # 查找包含Jerry内容的标签数目 可利用len()获得 # bsObj.findAll(id= "text") 等同于bsObj.findAll("",&#123;"id": "text"&#125;) 由于class是Python保留字，可能会出错 # print bsObj.findAll(&#123;"h1", "h2", "h3", "h4", "h5", "h6"&#125;) # span标签， 以及span标签对应的属性, 还有属性值。属性:属性值 通过字典实现 except AttributeError as e: return None for name in nameList: print name.get_text() # get_text()去掉所有标签# 标签在文档中的位置来查找标签 导航树# 不是bs4函数总是处理当前标签的后代标签，子只有一个，后代有多个# bsObj.div.findAll("img") 会找出文档中第一个div标签，然后获得这个div后代里所有的行标签列表def findChild(url): try: html = urlopen(url) except HTTPError as e: return None try: bsObj = BeautifulSoup(html, "html.parser") print(bsObj.find("img",&#123;"src":"../img/gifts/img1.jpg"&#125;).parent.previous_sibling.get_text()) except AttributeError as e: return None for child in bsObj.find("table",&#123;"id": "giftList"&#125;).children: # 如果把.children 换成 .descendants 会产生所有后代 # 换成.tr.next_sibings（兄弟标签: 只会返回后面的兄弟标签, 所以选择标签行然后调用next_siblings）与previous_siblings相反 处理表格数据更简单 # 只使用标签容易丢失细节不稳定 ，尽量利用标签属性 # print child pass# 正则表达式 regular expression: regex 经典应用识别邮箱地址# 词组正则字符串# aa*至少出现一次1~n个a# (cc)*出现0~2n次cc# (d | )出现跟着空格的d或者只有空格# 邮箱格式： [A-Za-z0-9\._+]+@[A-Za-z]+\.(com|org|edu|net)# bs4 与 正则表达式 配合使用# find("img")明显不行 会有隐藏的图片def findImg(url): try: html = urlopen(url) except AttributeError as e: return None try: bsObj = BeautifulSoup(html) images = bsObj.findAll("image",&#123;"src":re.compile("\.\.\/img\/gifts/img.*\.jpg")&#125;) for image in images: print (image["src"])# 打印出图片的相对路径 except HTTPError as e: return None# myTag.attrs 返回标签的全部属性，Python字典对象# 获取图片资源位置 ： myImgTag.attrs["src"]# Lambda函数可作为findAll()的参数，# 要求是标签作为参数，返回结果是bool类型# 获得有两个属性的标签 ： soup.findAll(lambda: tag: len(tag.attrs)==2)# content = getTitle("http://www.jb51.net/article/65303.htm")# content = useFind("http://www.pythonscraping.com/pages/warandpeace.html")content = findChild("http://www.pythonscraping.com/pages/page3.html")if content is None: print "title not found"else: print content]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[deep learning]]></title>
      <url>%2F2017%2F04%2F24%2Fdeep-learning%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[python学习]]></title>
      <url>%2F2017%2F04%2F23%2Fpython%E5%AD%A6%E4%B9%A0%2F</url>
      <content type="text"><![CDATA[##python 的数据类型 numbers 123451. int:2. long3. float4. complex5. string list 1列表用[]表示，类似数组。但是它可含有多种数据类型的数据 tuple 1不能二次赋值，相当于只读列表。 dictionary 1理解键的含义]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[作息时间表]]></title>
      <url>%2F2017%2F04%2F23%2F%E4%BD%9C%E6%81%AF%E6%97%B6%E9%97%B4%E8%A1%A8%2F</url>
      <content type="text"><![CDATA[#考研作息时间表[TOC] 7:00 起床7:30 吃早饭8:00-9:00 背英语单词9:00-11:00 复习数学11:00-11;40 看英文周刊11:40-13:00 午饭和午休,洗脸养好精神13:00-17:00 复习专业课17:00-17:30 晚饭17:30-19:00 刷牛客网pat19:00-21:00 刷数学题目21:00-24:00 刷真题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于PageRank的理解]]></title>
      <url>%2F2017%2F04%2F21%2F%E5%85%B3%E4%BA%8EPageRank%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[##google大法：PageRank的了解在网络检索课程上了解了PageRank这个东西，也是google为什么能在第二次搜索引擎竞争活下来的原因。PageRank由google创始人Larry Page发明和命名的。 主要目的是去measure the importance of website page。越多的链接指向的页面，则越重要。(这就好比社会上，某个领域的专家总是比门外汉对本专业有更大的权威性）google如此解释： 12345PageRank works by counting the number and quality of links to a page to determine a rough estimate of how importantthe website is. The underlying assumption is that more important websites are likely to receive more links from other websites]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[近期目标]]></title>
      <url>%2F2017%2F04%2F21%2F%E8%BF%91%E6%9C%9F%E7%9B%AE%E6%A0%87%2F</url>
      <content type="text"><![CDATA[#近期要完成的东西： 牛客网pat刷够20题准备报考pat 复习考研数学和专业课和英语（真是每天必备的），现在以刷题静心为目标。 近期有os课程设计，网络检索实验，数值计算实验。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo优化]]></title>
      <url>%2F2017%2F04%2F20%2Fhexo%E4%BC%98%E5%8C%96%2F</url>
      <content type="text"><![CDATA[这几天一直在折腾自己的blog通过Hexo框架，选择排名第一的nexT。 应金洲学长的要求添加了评论功能(由于多说要关闭了不得不选其他的网站有的丑，有的必须翻墙，有的还需要备案，真是心累） 后来通过网上的教程又添加了查询功能，标签，网站图标等等。总体来说后期越来越得心应手，主要是作者已经把很多功能都集成好了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[第一篇博客]]></title>
      <url>%2F2017%2F04%2F19%2Fhello-world%2F</url>
      <content type="text"><![CDATA[今天终于用Hexo搭建好了博客，刚开始操作起来真是麻烦，不过后期熟练之后各种得心应手，感觉自己搭载博客还是挺牛的，哈哈。 Hexo的几种命令Create a new post（放在_post文件夹下）123$ hexo new "My New Post"//这个是新建page$hexo new page "tag" More info: Writing 跑本地服务器123$ hexo server//或者$hexo s --debug More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites12$ hexo deploy//不知道为什么，发布到github必须先用hexo g, 然后 hexo d More info: Deployment 搭建hexo+github所有命令12345$ npm install hexo -g #安装 $ npm update hexo -g #升级 $ npm install hexo --save $ hexo init$ npm install]]></content>
    </entry>

    
  
  
</search>
